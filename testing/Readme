These directions for semi-automated testing are similar to those for 
homework 3. The big differences are that we are giving you a working
lexer and parser that you can optionally use if yours is not yet
working and that the main comparisons are now of the expected error
messages, not the abstract syntax that is generated. However, the
testing regime includes a comparison of the abstract syntax, even
though you should be past this testing with homework 3. 

Here are the specific instructions for using the automated testing
framework: 

1. Edit the first two lines of the runsml file to indicate whether
or not you have a working lexer and parser and the next three to
indicate the correct directories for the source code, the test cases
directory and the current directory. (Typically, you will want to
leave the last as is.)

As before, remember that you should not use ~ in paths. You can use
.., though.  

Remember also that the test cases for which we have generated outputs 
are all those in the syntax-and-sem-analysis directory, so it is only
your relevant path to this directory that you should use for the 
automated testing. 

2. Make sure you do not have a file named parse.sml.bak in the code 
directory that you want to preserve; if you do, move it somewhere else
(or change the lines in runsml that make use of this name to save a
copy of the original parse.sml file).

3. Run ./runsml. This will run the driver for all files in the testcases 
directory. Doing so will generate a script of whatever is intended to
be displayed on the monitor/terminal in the file my_script_output and
will compare this to the ones generated by the baseline program that
is in baseline_output_script. Additionally, the abstract syntax that
is generated will be compared with that of the baseline program. 

Note that using diff on the output scripts is a very approximate
way of checking that your semantic analyzer is doing well. Error
messages usually have a personal touch and so your error messages
are likely to be different from mine. When you look at the result
of the diff, mainly check that errors are being reported for the
same programs by both your semantic analyzer and mine and also
check that they are catching the same errors, even if the messages
are different. (You may also look at my error messages to see if you
are happy with the ones your semantic analyzer is providing, of
course.)

Note also that the comparison of the abstract syntax could be
superfluous at this stage, assuming you have a working parser. (It is
even more superfluous if you don't, because then you will be comparing
the output of my parser with itself.) In this case, you may want to
comment out the relevant parts of the runsml script.

4. If you don't want to run the test for all files, remove the line 
in runsml which creates a listing of the files in the testing directory 
in the file "files.list" and create your own  "files.list" with the 
names of the testcases you need. The diff will show some errors like 
"only present in correct_results".  This is because now we are producing 
the results of only the files in "files.list".

5. If you find it difficult to understand the output of the diff tool on 
the terminal, you could instead use visual diff tools, such as 'meld'. 
Linux machines in the CSE domain should have this tool. You can integrate 
this into the runsml script by replacing lines:

	diff baseline_output_script my_output_script
	diff correct_outputs outputs

with:
	meld baseline_output_script my_output_script &
	meld correct_outputs/ outputs/ &

which will start the GUI interface of meld for comparing the files. If you 
are connecting to these machine remotely using ssh, you will have to use 
the -X flag as shown:

	ssh -X name@machine.cs.umn.edu
